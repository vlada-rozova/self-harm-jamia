{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vrozova/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, fbeta_score, average_precision_score\n",
    "\n",
    "import nlp_utils as utils\n",
    "from nlp_utils import get_vectorizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Embedding, Bidirectional\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import Callback, ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "from keras import metrics\n",
    "\n",
    "# import tensorflow.python.util.deprecation as deprecation\n",
    "# deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-ticks')\n",
    "sns.set_style('ticks')\n",
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "plt.rcParams['axes.titlesize'] = 22\n",
    "plt.rcParams['axes.labelsize'] = 20\n",
    "plt.rcParams['xtick.labelsize'] = 16\n",
    "plt.rcParams['ytick.labelsize'] = 16\n",
    "\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "text = \"entities\"\n",
    "class_names = (\"Controls\", \"Self harm\")\n",
    "\n",
    "if len(class_names) == 2:\n",
    "    average = \"binary\"\n",
    "else:\n",
    "    average = \"macro\"\n",
    "    \n",
    "# Undersampling\n",
    "undersample = False\n",
    "n_controls = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    307875\n",
      "1      4302\n",
      "Name: SH, dtype: int64\n",
      "\n",
      "Max triage length: 67\n",
      "Using 5000 words\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data/rmh_train.csv\")\n",
    "\n",
    "print(df_train.SH.value_counts())\n",
    "print()\n",
    "\n",
    "triage_length = df_train[text].apply(lambda x: len(x.split())).max()\n",
    "print(\"Max triage length:\", triage_length)\n",
    "num_words = 5000\n",
    "print(\"Using %d words\" % num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    embed_dim = 50\n",
    "    lstm_out = 25\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, embed_dim, input_length=triage_length))\n",
    "#     model.add(Bidirectional(LSTM(lstm_out)))\n",
    "    model.add(LSTM(lstm_out, recurrent_dropout=0.2, dropout=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', \n",
    "                  optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Model saving callback\n",
    "ckpt_callback = ModelCheckpoint('models/keras_model', \n",
    "                                 monitor='val_loss', \n",
    "                                 verbose=1, \n",
    "                                 save_best_only=True, \n",
    "                                 mode='auto')\n",
    "# TensorBoard logs\n",
    "logdir = \"logs/\" + datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Custom metrics\n",
    "class CustomMetrics(Callback):\n",
    "    def __init__(self, val_data):\n",
    "        super().__init__()\n",
    "        self.validation_data = val_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_proba = self.model.predict(self.validation_data[0])\n",
    "        if y_proba.shape[1] == 1:\n",
    "            y_pred = np.where(y_proba > 0.5, 1, 0)\n",
    "        else:\n",
    "            y_pred = np.argmax(y_proba, axis=1)\n",
    "        y = self.validation_data[1]\n",
    "        \n",
    "        logs['val_precision'] = precision_score(y, y_pred, average=average)\n",
    "        logs['val_recall'] = recall_score(y, y_pred, average=average)\n",
    "        logs['val_f1'] = f1_score(y, y_pred, average=average)\n",
    "#         logs['val_f2'] = fbeta_score(y, y_pred, average=average, beta=2)\n",
    "        logs['val_ap'] = average_precision_score(y, y_proba)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 67, 50)            250000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 25)                7600      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 257,626\n",
      "Trainable params: 257,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(df_train, test_size=0.1, random_state=42, stratify=df_train.SH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280959, 67)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=num_words, filters=\"\", split=\" \")\n",
    "tokenizer.fit_on_texts(df_train[text].values)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(df_train[text].values)\n",
    "X_train = pad_sequences(X_train, maxlen=triage_length)\n",
    "\n",
    "y_train = df_train.SH.values\n",
    "\n",
    "class_weight = dict(zip((0,1), y_train.shape[0] / (len(class_names) * np.bincount(y_train))))\n",
    "\n",
    "X_val = tokenizer.texts_to_sequences(df_val[text].values)\n",
    "X_val = pad_sequences(X_val, maxlen=triage_length)\n",
    "\n",
    "y_val = df_val.SH.values\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4390/4390 [==============================] - 251s 57ms/step - loss: 0.1657 - val_loss: 0.1031\n",
      "Epoch 2/10\n",
      "4390/4390 [==============================] - 251s 57ms/step - loss: 0.0867 - val_loss: 0.0651\n",
      "Epoch 3/10\n",
      "4390/4390 [==============================] - 253s 58ms/step - loss: 0.0631 - val_loss: 0.0922\n",
      "Epoch 4/10\n",
      "4390/4390 [==============================] - 254s 58ms/step - loss: 0.0511 - val_loss: 0.0574\n",
      "Epoch 5/10\n",
      "4390/4390 [==============================] - 242s 55ms/step - loss: 0.0431 - val_loss: 0.0899\n",
      "Epoch 6/10\n",
      "4390/4390 [==============================] - 241s 55ms/step - loss: 0.0377 - val_loss: 0.0458\n",
      "Epoch 7/10\n",
      "4390/4390 [==============================] - 242s 55ms/step - loss: 0.0336 - val_loss: 0.0395\n",
      "Epoch 8/10\n",
      "4390/4390 [==============================] - 241s 55ms/step - loss: 0.0326 - val_loss: 0.0559\n",
      "Epoch 9/10\n",
      "4390/4390 [==============================] - 241s 55ms/step - loss: 0.0288 - val_loss: 0.0594\n",
      "Epoch 10/10\n",
      "4390/4390 [==============================] - 241s 55ms/step - loss: 0.0279 - val_loss: 0.0458\n",
      "CPU times: user 1h 2min 36s, sys: 2min 39s, total: 1h 5min 15s\n",
      "Wall time: 40min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "metrics = CustomMetrics((X_val, y_val))\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=n_epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    callbacks=[metrics], \n",
    "                    class_weight=class_weight,\n",
    "                    workers=2,\n",
    "                    verbose=1,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AP score: 0.856\n"
     ]
    }
   ],
   "source": [
    "print(\" AP score: %0.3f\" % np.max(history.history[\"val_ap\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.16567039489746094,\n",
       "  0.08669167011976242,\n",
       "  0.06305905431509018,\n",
       "  0.05109117552638054,\n",
       "  0.04311368986964226,\n",
       "  0.03767296299338341,\n",
       "  0.03355661407113075,\n",
       "  0.032605115324258804,\n",
       "  0.028774309903383255,\n",
       "  0.027943145483732224],\n",
       " 'val_loss': [0.10312473773956299,\n",
       "  0.06511218100786209,\n",
       "  0.0922049731016159,\n",
       "  0.05740988627076149,\n",
       "  0.08994237333536148,\n",
       "  0.04582354053854942,\n",
       "  0.039464615285396576,\n",
       "  0.055877696722745895,\n",
       "  0.059398118406534195,\n",
       "  0.04582565277814865],\n",
       " 'val_precision': [0.2982078853046595,\n",
       "  0.3856332703213611,\n",
       "  0.30307467057101023,\n",
       "  0.4091816367265469,\n",
       "  0.30354505169867063,\n",
       "  0.45601851851851855,\n",
       "  0.5104438642297651,\n",
       "  0.4091836734693878,\n",
       "  0.4064711830131446,\n",
       "  0.5006451612903225],\n",
       " 'val_recall': [0.9674418604651163,\n",
       "  0.9488372093023256,\n",
       "  0.9627906976744186,\n",
       "  0.9534883720930233,\n",
       "  0.9558139534883721,\n",
       "  0.9162790697674419,\n",
       "  0.9093023255813953,\n",
       "  0.9325581395348838,\n",
       "  0.9348837209302325,\n",
       "  0.9023255813953488],\n",
       " 'val_f1': [0.4558904109589041,\n",
       "  0.5483870967741936,\n",
       "  0.4610244988864142,\n",
       "  0.5726256983240223,\n",
       "  0.460762331838565,\n",
       "  0.6089644513137558,\n",
       "  0.6538461538461539,\n",
       "  0.5687943262411348,\n",
       "  0.5665961945031712,\n",
       "  0.6439834024896265],\n",
       " 'val_ap': [0.7764862662660761,\n",
       "  0.8334426436696215,\n",
       "  0.796211282610168,\n",
       "  0.8360329073650047,\n",
       "  0.8350990055626897,\n",
       "  0.8414188753405765,\n",
       "  0.8518170951408158,\n",
       "  0.855613083188511,\n",
       "  0.8334243398130349,\n",
       "  0.8326694511666345]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10, 8)\n",
    "plt.plot(range(n_epochs), history.history[\"loss\"], label=\"Train loss\");\n",
    "plt.plot(range(n_epochs), history.history[\"val_loss\"], label=\"Val loss\");\n",
    "plt.xlabel(\"Epochs\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10, 8)\n",
    "plt.plot(range(n_epochs), history.history[\"val_ap\"], label=\"Val AP\");\n",
    "plt.xlabel(\"Epochs\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('models/keras_model_1000')\n",
    "y_proba = model.predict(X_val)\n",
    "y_val = df_val.SH.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final model: %s\" % model)\n",
    "print(\"Using %d features.\" % num_words)\n",
    "if undersample:\n",
    "    print(\"Trained on %d controls.\\n\" % n_controls)\n",
    "utils.evaluate_model(y_val, y_proba, class_names, \"validation\", digits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(312177, 67)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=num_words, filters=\"\", split=\" \")\n",
    "tokenizer.fit_on_texts(df_train[text].values)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df_train[text].values)\n",
    "X = pad_sequences(X, maxlen=triage_length)\n",
    "\n",
    "y = df_train.SH.values\n",
    "\n",
    "class_weight = dict(zip((0,1), y.shape[0] / (len(class_names) * np.bincount(y))))\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3252/3252 - 340s - loss: 0.1822 - val_loss: 0.1252\n",
      "Epoch 2/10\n",
      "3252/3252 - 340s - loss: 0.0794 - val_loss: 0.0989\n",
      "Epoch 3/10\n",
      "3252/3252 - 338s - loss: 0.0541 - val_loss: 0.0845\n",
      "Epoch 4/10\n",
      "3252/3252 - 333s - loss: 0.0424 - val_loss: 0.0660\n",
      "Epoch 5/10\n",
      "3252/3252 - 312s - loss: 0.0353 - val_loss: 0.0499\n",
      "Epoch 6/10\n",
      "3252/3252 - 306s - loss: 0.0314 - val_loss: 0.0741\n",
      "Epoch 7/10\n",
      "3252/3252 - 308s - loss: 0.0281 - val_loss: 0.0459\n",
      "Epoch 8/10\n",
      "3252/3252 - 307s - loss: 0.0226 - val_loss: 0.0426\n",
      "Epoch 9/10\n",
      "3252/3252 - 304s - loss: 0.0224 - val_loss: 0.0613\n",
      "Epoch 10/10\n",
      "3252/3252 - 304s - loss: 0.0183 - val_loss: 0.0447\n",
      "Epoch 1/10\n",
      "3252/3252 - 313s - loss: 0.1750 - val_loss: 0.1352\n",
      "Epoch 2/10\n",
      "3252/3252 - 309s - loss: 0.0858 - val_loss: 0.1023\n",
      "Epoch 3/10\n",
      "3252/3252 - 308s - loss: 0.0595 - val_loss: 0.0533\n",
      "Epoch 4/10\n",
      "3252/3252 - 303s - loss: 0.0437 - val_loss: 0.0571\n",
      "Epoch 5/10\n",
      "3252/3252 - 297s - loss: 0.0394 - val_loss: 0.0427\n",
      "Epoch 6/10\n",
      "3252/3252 - 298s - loss: 0.0323 - val_loss: 0.0417\n",
      "Epoch 7/10\n",
      "3252/3252 - 296s - loss: 0.0257 - val_loss: 0.0582\n",
      "Epoch 8/10\n",
      "3252/3252 - 297s - loss: 0.0263 - val_loss: 0.0387\n",
      "Epoch 9/10\n",
      "3252/3252 - 298s - loss: 0.0237 - val_loss: 0.0596\n",
      "Epoch 10/10\n",
      "3252/3252 - 297s - loss: 0.0246 - val_loss: 0.0330\n",
      "Epoch 1/10\n",
      "3252/3252 - 303s - loss: 0.1714 - val_loss: 0.1317\n",
      "Epoch 2/10\n",
      "3252/3252 - 300s - loss: 0.0800 - val_loss: 0.0797\n",
      "Epoch 3/10\n",
      "3252/3252 - 301s - loss: 0.0513 - val_loss: 0.0663\n",
      "Epoch 4/10\n",
      "3252/3252 - 302s - loss: 0.0408 - val_loss: 0.0456\n",
      "Epoch 5/10\n",
      "3252/3252 - 302s - loss: 0.0337 - val_loss: 0.0497\n",
      "Epoch 6/10\n",
      "3252/3252 - 301s - loss: 0.0290 - val_loss: 0.0408\n",
      "Epoch 7/10\n",
      "3252/3252 - 301s - loss: 0.0273 - val_loss: 0.0683\n",
      "Epoch 8/10\n",
      "3252/3252 - 300s - loss: 0.0242 - val_loss: 0.0550\n",
      "Epoch 9/10\n",
      "3252/3252 - 301s - loss: 0.0206 - val_loss: 0.0493\n",
      "Epoch 10/10\n",
      "3252/3252 - 302s - loss: 0.0219 - val_loss: 0.0566\n",
      "CPU times: user 4h 13min 8s, sys: 7min 26s, total: 4h 20min 34s\n",
      "Wall time: 2h 33min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_splits = 3\n",
    "n_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "cv = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "cv_history = []\n",
    "\n",
    "for train_index, val_index in cv.split(X, y):\n",
    "    model=create_model()\n",
    "    \n",
    "    metrics = CustomMetrics((X[val_index], y[val_index]))\n",
    "    \n",
    "    history = model.fit(X[train_index], y[train_index], \n",
    "                    epochs=n_epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=(X[val_index], y[val_index]), \n",
    "                    callbacks=[metrics], \n",
    "                    class_weight=class_weight,\n",
    "                    workers=2,\n",
    "                    verbose=2,\n",
    "                   )\n",
    "    \n",
    "    cv_history.append(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_splits):\n",
    "    sns.lineplot(x=range(1,n_epochs+1), y=cv_history[i]['val_recall']);\n",
    "    \n",
    "plt.ylabel(\"Recall\");\n",
    "plt.xlabel(\"Epochs\");\n",
    "plt.xticks((1,2,3));\n",
    "plt.title(\"Five-fold cross-validation\");\n",
    "plt.ylim([0.8, 1]);\n",
    "plt.savefig(\"cv_rec.png\", bbox_inches='tight', dpi=300, transparent=False, pad_inches=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.560 (+/- 0.08)\n",
      "Average Recall: 0.874 (+/- 0.02)\n",
      "Average F1 score: 0.682 (+/- 0.05)\n",
      "Average AP score: 0.801 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "def get_final_score(metric):\n",
    "    scores = []\n",
    "    for i in range(n_splits):\n",
    "        scores.append(cv_history[i][metric][-1])\n",
    "    return np.array(scores)\n",
    "\n",
    "print(\"Average Precision: %0.3f (+/- %0.2f)\" % (get_final_score(\"val_precision\").mean(), \n",
    "                                                get_final_score(\"val_precision\").std() * 2))\n",
    "print(\"Average Recall: %0.3f (+/- %0.2f)\" % (get_final_score(\"val_recall\").mean(), \n",
    "                                             get_final_score(\"val_recall\").std() * 2))\n",
    "print(\"Average F1 score: %0.3f (+/- %0.2f)\" % (get_final_score(\"val_f1\").mean(), \n",
    "                                               get_final_score(\"val_f1\").std() * 2))\n",
    "print(\"Average AP score: %0.3f (+/- %0.2f)\" % (get_final_score(\"val_ap\").mean(), \n",
    "                                               get_final_score(\"val_ap\").std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_final_score(\"loss\"), get_final_score(\"loss\").mean(), get_final_score(\"loss\").std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_final_score(\"val_loss\"), get_final_score(\"val_loss\").mean(), get_final_score(\"val_loss\").std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I don't know if it's a thing but can I train another model to reclassify presentations predicted as either SI or SH? Which dataset would I use for it, training or validation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "y_pred.shape, (y_pred > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_cases = df_val[y_pred > 0].copy()\n",
    "df_pred_cases[\"y_pred_1\"] = y_pred[y_pred > 0]\n",
    "df_pred_cases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_pred_cases.y.values\n",
    "\n",
    "vectorizer = FeatureSelector(params)\n",
    "X = vectorizer.fit_transform(df_pred_cases[data], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class=\"ovr\", class_weight=\"balanced\")\n",
    "utils.benchmark_cv_score(clf, X, y, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = FeatureSelector(params)\n",
    "pipe = make_pipeline(vectorizer, clf)\n",
    "pipe.fit(df_pred_cases[data], y)\n",
    "y_proba = pipe.predict_proba(df_pred_cases[data])\n",
    "utils.evaluate_model(y, y_proba, class_names, \"training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

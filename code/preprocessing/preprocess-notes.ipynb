{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Five preprocessing steps:\n",
    "This notebook contains the main preprocessing steps to clean triage notes:\n",
    "1. Pre-processing\n",
    "2. Tokenization\n",
    "3. Re-tokenization\n",
    "4. Post-processing\n",
    "5. Spelling correction\n",
    "6. Slang replacement\n",
    "\n",
    "The logic of the notebook is non-linear, i.e. it provides the input for and uses the output of other notebooks and should be excecuted according to the flowchart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "import pickle\n",
    "import time\n",
    "from nlp_utils import preprocess, find_pattern\n",
    "from custom_tokenizer import combined_rule_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57681, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:15:00</td>\n",
       "      <td>argument with friend, threatened to jump off b...</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 00:20:00</td>\n",
       "      <td>mech fall with swelling to L) hand and dec ROM...</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01 00:33:00</td>\n",
       "      <td>Left lower dental pain since last year, seekin...</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01 00:34:00</td>\n",
       "      <td>ETOH, scuffle with HS ? LOC, lac approx 2cm ab...</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01 00:36:00</td>\n",
       "      <td>mech fall landed L) hip. headstrike onto wall....</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp                                               text  \\\n",
       "0  2020-01-01 00:15:00  argument with friend, threatened to jump off b...   \n",
       "1  2020-01-01 00:20:00  mech fall with swelling to L) hand and dec ROM...   \n",
       "2  2020-01-01 00:33:00  Left lower dental pain since last year, seekin...   \n",
       "3  2020-01-01 00:34:00  ETOH, scuffle with HS ? LOC, lac approx 2cm ab...   \n",
       "4  2020-01-01 00:36:00  mech fall landed L) hip. headstrike onto wall....   \n",
       "\n",
       "   length  \n",
       "0     112  \n",
       "1      60  \n",
       "2     142  \n",
       "3     145  \n",
       "4      84  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv(\"../data/rmh_raw.csv\")\n",
    "df = pd.read_csv(\"../data/epic2020.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess comments\n",
    "Preprocess to handle errors in data extraction and some abbreviations specific to triage notes.\n",
    "* `\\x7f`\n",
    "* `'/c`\n",
    "* `l)` as \"left\", `r)` as \"right\"\n",
    "* `@` as \"at\"\n",
    "* `#` as \"fractured\"\n",
    "* `++ve` as \"positive\", `--ve` as \"negative\"\n",
    "* etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.2 s, sys: 9.54 ms, total: 6.21 s\n",
      "Wall time: 6.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preprocess comments\n",
    "df['text_clean'] = df.text.apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/orygen/code/sh_env_updated/lib/python3.6/site-packages/spacy/util.py:762: UserWarning: [W095] Model 'en_core_sci_sm' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  target (Model): The target node.\n"
     ]
    }
   ],
   "source": [
    "# Load scispacy model for tokenization\n",
    "nlp = spacy.load(\"en_core_sci_sm\", disable=['tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner'])\n",
    "nlp.tokenizer = combined_rule_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.1 s, sys: 1.59 s, total: 56.7 s\n",
      "Wall time: 56.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['text_clean'] = list(nlp.pipe(df.text_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_multiple_tokens(string):\n",
    "    pattern = re.compile(\".[-/\\+_,\\?\\.].\")\n",
    "    return pattern.search(string) and string not in vocab\n",
    "\n",
    "def retokenize(text):\n",
    "    new_text = []\n",
    "    for token in text:\n",
    "        if token.like_num:\n",
    "            new_text.append(token.text)\n",
    "        elif is_multiple_tokens(token.text):\n",
    "            [new_text.append(new_token) for new_token in re.split('([-/\\+_,\\?\\.])', token.text)]\n",
    "        else:\n",
    "            new_text.append(token.text)\n",
    "            \n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a custom word frequency list\n",
    "with open ('../data/spelling_correction/rmh_custom_vocab.txt', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "    \n",
    "# Initialise spellchecker with a custom vocab\n",
    "spell = SpellChecker(language=None)\n",
    "spell.word_frequency.load_words(vocab)\n",
    "\n",
    "print(\"Custom vocabulary contains a total of %d words and %d unique words.\" % \n",
    "      (len(vocab), len(set(vocab))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.text_clean = df.text_clean.apply(retokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"\\s\\.([a-z]{2,})\")\n",
    "\n",
    "df.text_clean = df.text_clean.apply(lambda x: pattern.sub(r\" . \\1\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/spelling_correction/rmh_nospellcorr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct spelling in triage notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(486458, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SH</th>\n",
       "      <th>SI</th>\n",
       "      <th>length</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>140</td>\n",
       "      <td>SOB for 5/7, been to GP given prednisolone, co...</td>\n",
       "      <td>sob for 5/7 , been to gp given prednisolone , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107</td>\n",
       "      <td>pt has lac down right forehead, to eyebrow, wi...</td>\n",
       "      <td>pt has lac down right forehead , to eyebrow , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74</td>\n",
       "      <td>pt expect MBA, trapped for 45mins, #right femu...</td>\n",
       "      <td>pt expect mba , trapped for 45 mins , fracture...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>167</td>\n",
       "      <td>L) sided flank pain same as previous renal col...</td>\n",
       "      <td>left sided flank pain same as previous renal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>193</td>\n",
       "      <td>generalised abdo pain and associated headache ...</td>\n",
       "      <td>generalised abdo pain and associated headache ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SH  SI  length                                               text  \\\n",
       "0  0.0 NaN     140  SOB for 5/7, been to GP given prednisolone, co...   \n",
       "1  0.0 NaN     107  pt has lac down right forehead, to eyebrow, wi...   \n",
       "2  0.0 NaN      74  pt expect MBA, trapped for 45mins, #right femu...   \n",
       "3  0.0 NaN     167  L) sided flank pain same as previous renal col...   \n",
       "4  0.0 NaN     193  generalised abdo pain and associated headache ...   \n",
       "\n",
       "                                          text_clean  \n",
       "0  sob for 5/7 , been to gp given prednisolone , ...  \n",
       "1  pt has lac down right forehead , to eyebrow , ...  \n",
       "2  pt expect mba , trapped for 45 mins , fracture...  \n",
       "3    left sided flank pain same as previous renal...  \n",
       "4  generalised abdo pain and associated headache ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/spelling_correction/rmh_nospellcorr.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionary of misspelled words and their corrections\n",
    "with open ('../data/spelling_correction/rmh_misspelled_dict.txt', 'rb') as f:\n",
    "    misspelled = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spelling_correction(doc):\n",
    "    tokens = doc.text.split()\n",
    "    corrected_tokens = [misspelled[token][1] if token in misspelled else token for token in tokens]\n",
    "    return ' '.join(corrected_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.18 s, sys: 9.91 ms, total: 1.19 s\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.text_clean = df.text_clean.apply(spelling_correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace slang drug names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drugs = pd.read_csv(\"../data/spelling_correction/medication_names.csv\")\n",
    "\n",
    "df_drugs.slang = df_drugs.slang.str.strip().str.lower()\n",
    "df_drugs.generic_name = df_drugs.generic_name.str.strip().str.lower()\n",
    "df_drugs.dropna(subset=[\"slang\"], inplace=True)\n",
    "\n",
    "slang_names = dict(zip(df_drugs.slang, df_drugs.generic_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slang_to_generic(doc):\n",
    "    tokens = doc.split()\n",
    "    corrected_tokens = [slang_names[token] if token in slang_names else token for token in tokens]\n",
    "    return ' '.join(corrected_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 301 ms, sys: 9.04 ms, total: 310 ms\n",
      "Wall time: 310 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.text_clean = df.text_clean.apply(slang_to_generic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/epic2020_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How much did this reduce the dimensionality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"../data/rmh_raw.csv\")\n",
    "df2 = pd.read_csv(\"../data/spelling_correction/rmh_nospellcorr.csv\")\n",
    "df2 = df2[:466605]\n",
    "df3 = pd.read_csv(\"../data/rmh_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29778"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(df.text)\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26341"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(df.text_clean)\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(df3.text_clean)\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100328 - 43887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "94326 - 60561"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/rmh_raw.csv\")\n",
    "df_train = pd.read_csv(\"../data/rmh_raw_train.csv\")\n",
    "df_test = pd.read_csv(\"../data/rmh_raw_test.csv\")\n",
    "df_ho = pd.read_csv(\"../data/rmh_raw_holdout.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, df_train.shape, df_test.shape, df_ho.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_print(df):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(df.text)\n",
    "    \n",
    "    lexical_diversity = X.shape[1] / X.sum().sum()\n",
    "    print(\"Vocabulary size: \", X.shape[1])\n",
    "    print(\"Total number of tokens: \", X.sum().sum())\n",
    "    print(\"Lexical diversity: \", lexical_diversity)\n",
    "    print(\"Total number of reviews:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_print(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_print(df_ho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "312177 + 78045"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

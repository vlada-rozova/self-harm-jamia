{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Five preprocessing steps:\n",
    "This notebook contains the main preprocessing steps to clean triage notes:\n",
    "1. Pre-processing\n",
    "2. Tokenization\n",
    "3. Re-tokenization\n",
    "4. Post-processing\n",
    "5. Spelling correction\n",
    "6. Slang replacement\n",
    "\n",
    "The logic of the notebook is non-linear, i.e. it provides the input for and uses the output of other notebooks and should be excecuted according to the flowchart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "import pickle\n",
    "import time\n",
    "from nlp_utils import preprocess, find_pattern\n",
    "from custom_tokenizer import combined_rule_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"../data/rmh_raw.csv\")\n",
    "df = pd.read_csv(\"../../data/rmh_raw.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess comments\n",
    "Preprocess to handle errors in data extraction and some abbreviations specific to triage notes.\n",
    "* `\\x7f`\n",
    "* `'/c`\n",
    "* `l)` as \"left\", `r)` as \"right\"\n",
    "* `@` as \"at\"\n",
    "* `#` as \"fractured\"\n",
    "* `++ve` as \"positive\", `--ve` as \"negative\"\n",
    "* etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Preprocess comments\n",
    "df['text_clean'] = df.text.apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scispacy model for tokenization\n",
    "nlp = spacy.load(\"en_core_sci_sm\", disable=['tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner'])\n",
    "nlp.tokenizer = combined_rule_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df['text_clean'] = list(nlp.pipe(df.text_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_multiple_tokens(string):\n",
    "    pattern = re.compile(\".[-/\\+_,\\?\\.].\")\n",
    "    return pattern.search(string) and string not in vocab\n",
    "\n",
    "def retokenize(text):\n",
    "    new_text = []\n",
    "    for token in text:\n",
    "        if token.like_num:\n",
    "            new_text.append(token.text)\n",
    "        elif is_multiple_tokens(token.text):\n",
    "            [new_text.append(new_token) for new_token in re.split('([-/\\+_,\\?\\.])', token.text)]\n",
    "        else:\n",
    "            new_text.append(token.text)\n",
    "            \n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a custom word frequency list\n",
    "with open (\"../../data/spelling_correction/rmh_custom_vocab.txt\", 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "    \n",
    "# Initialise spellchecker with a custom vocab\n",
    "spell = SpellChecker(language=None)\n",
    "spell.word_frequency.load_words(vocab)\n",
    "\n",
    "print(\"Custom vocabulary contains a total of %d words and %d unique words.\" % \n",
    "      (len(vocab), len(set(vocab))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.text_clean = df.text_clean.apply(retokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"\\s\\.([a-z]{2,})\")\n",
    "\n",
    "df.text_clean = df.text_clean.apply(lambda x: pattern.sub(r\" . \\1\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../data/spelling_correction/rmh_nospellcorr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct spelling in triage notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/spelling_correction/rmh_nospellcorr.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionary of misspelled words and their corrections\n",
    "with open (\"../../data/spelling_correction/rmh_misspelled_dict.txt\", 'rb') as f:\n",
    "    misspelled = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spelling_correction(doc):\n",
    "    tokens = doc.text.split()\n",
    "    corrected_tokens = [misspelled[token][1] if token in misspelled else token for token in tokens]\n",
    "    return ' '.join(corrected_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.text_clean = df.text_clean.apply(spelling_correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace slang drug names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drugs = pd.read_csv(\"../../data/spelling_correction/medication_names.csv\")\n",
    "\n",
    "df_drugs.slang = df_drugs.slang.str.strip().str.lower()\n",
    "df_drugs.generic_name = df_drugs.generic_name.str.strip().str.lower()\n",
    "df_drugs.dropna(subset=[\"slang\"], inplace=True)\n",
    "\n",
    "slang_names = dict(zip(df_drugs.slang, df_drugs.generic_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slang_to_generic(doc):\n",
    "    tokens = doc.split()\n",
    "    corrected_tokens = [slang_names[token] if token in slang_names else token for token in tokens]\n",
    "    return ' '.join(corrected_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.text_clean = df.text_clean.apply(slang_to_generic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../data/rmh_cleaned.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

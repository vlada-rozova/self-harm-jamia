{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a word frequency list\n",
    "\n",
    "This notebook loads the vocabulary learned from the MIMIC-III free-text notes and uses it as a starting point to generate a custom word frequency list. We expand the vocabulary by adding names of common drugs (including generic, brand, and slang names) and local mental health organisations.\n",
    "The word frequency list is generated by parsing the whole dataset and appending to an empty list every word that is known to the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "import pickle\n",
    "import time\n",
    "from nlp_utils import preprocess, find_pattern\n",
    "from custom_tokenizer import combined_rule_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieve MIMIC vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vocab retreived from the Med7 model\n",
    "with open ('../../data/spelling_correction/med7_vocab.txt', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "# Create an empty spellchecker object and initialise it with MIMIC vocab retrieved from the Med7 model\n",
    "spell = SpellChecker(language=None)\n",
    "spell.word_frequency.load_words(vocab)\n",
    "\n",
    "print(\"MIMIC vocabulary contains %d unique words (%d words in total).\" % \n",
    "      (spell.word_frequency.unique_words, spell.word_frequency.total_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add names of drugs and local mental health organisations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drugs = pd.read_csv(\"../../data/spelling_correction/medication_names.csv\")\n",
    "\n",
    "generic_names = [\n",
    "    word\n",
    "    for line in df_drugs.generic_name.dropna().str.strip().str.lower().str.replace(\"&\", \" \").tolist() \n",
    "    for word in line.split()\n",
    "]\n",
    "\n",
    "brand_names = [\n",
    "    word \n",
    "    for line in df_drugs.brand_name.dropna().str.strip().str.lower().str.replace(\"&\", \" \").str.replace(\"\\n\", \" \").tolist()\n",
    "    for word in line.split()\n",
    "]\n",
    "\n",
    "slang_names = df_drugs.slang.dropna().str.strip().str.lower().unique().tolist()\n",
    "\n",
    "drug_names = set(generic_names + brand_names + slang_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell.word_frequency.load_words(drug_names)\n",
    "\n",
    "spell.word_frequency.load_words([\"ecatt\", \"orygen\", \"saapu\", \n",
    "                                \"unrousable\",\"batcall\",\"acopia\", \n",
    "                                \"daswest\",\"neurovasc\", \"vasc\", \"bibp\"])\n",
    "\n",
    "print(\"Extended vocabulary contains %d unique words (%d words in total).\" % \n",
    "      (spell.word_frequency.unique_words, spell.word_frequency.total_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load RMH data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/rmh_raw.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess and tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Preprocess comments\n",
    "df['text_clean'] = df.text.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scispacy model for tokenization\n",
    "nlp = spacy.load(\"en_core_sci_sm\", disable=['tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner'])\n",
    "nlp.tokenizer = combined_rule_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['text_clean'] = list(nlp.pipe(df.text_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select a subset of tokens present in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if a token is known and add it to the vocab\n",
    "def add_to_vocab(text):\n",
    "    vocab.extend(spell.known([token.text for token in text])) \n",
    "    \n",
    "# Apply the function to each triage comment\n",
    "vocab = []\n",
    "df.text_clean.apply(add_to_vocab)\n",
    "\n",
    "print(\"Domain-specific vocabulary contains %d unique words (%d words in total).\" % \n",
    "      (len(set(vocab)), len(vocab)))\n",
    "      \n",
    "with open(\"../../data/spelling_correction/rmh_custom_vocab.txt\", 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

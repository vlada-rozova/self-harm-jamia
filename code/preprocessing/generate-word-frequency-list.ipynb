{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a word frequency list\n",
    "\n",
    "This notebook loads the vocabulary learned from the MIMIC-III free-text notes and uses it as a starting point to generate a custom word frequency list. We expand the vocabulary by adding names of common drugs (including generic, brand, and slang names) and local mental health organisations.\n",
    "The word frequency list is generated by parsing the whole dataset and appending to an empty list every word that is known to the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vrozova/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "import pickle\n",
    "import time\n",
    "from nlp_utils import preprocess, find_pattern\n",
    "from custom_tokenizer import combined_rule_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieve MIMIC vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIMIC vocabulary contains 790195 unique words (1509550 words in total).\n"
     ]
    }
   ],
   "source": [
    "# Load the vocab retreived from the Med7 model\n",
    "with open ('../data/spelling_correction/med7_vocab.txt', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "# Create an empty spellchecker object and initialise it with MIMIC vocab retrieved from the Med7 model\n",
    "spell = SpellChecker(language=None)\n",
    "spell.word_frequency.load_words(vocab)\n",
    "\n",
    "print(\"MIMIC vocabulary contains %d unique words (%d words in total).\" % \n",
    "      (spell.word_frequency.unique_words, spell.word_frequency.total_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add names of drugs and local mental health organisations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drugs = pd.read_csv(\"../data/spelling_correction/medication_names.csv\")\n",
    "\n",
    "generic_names = [\n",
    "    word\n",
    "    for line in df_drugs.generic_name.dropna().str.strip().str.lower().str.replace(\"&\", \" \").tolist() \n",
    "    for word in line.split()\n",
    "]\n",
    "\n",
    "brand_names = [\n",
    "    word \n",
    "    for line in df_drugs.brand_name.dropna().str.strip().str.lower().str.replace(\"&\", \" \").str.replace(\"\\n\", \" \").tolist()\n",
    "    for word in line.split()\n",
    "]\n",
    "\n",
    "slang_names = df_drugs.slang.dropna().str.strip().str.lower().unique().tolist()\n",
    "\n",
    "drug_names = set(generic_names + brand_names + slang_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended vocabulary contains 790509 unique words (1510106 words in total).\n"
     ]
    }
   ],
   "source": [
    "spell.word_frequency.load_words(drug_names)\n",
    "\n",
    "spell.word_frequency.load_words([\"ecatt\", \"orygen\", \"saapu\", \n",
    "                                \"unrousable\",\"batcall\",\"acopia\", \n",
    "                                \"daswest\",\"neurovasc\", \"vasc\", \"bibp\"])\n",
    "\n",
    "print(\"Extended vocabulary contains %d unique words (%d words in total).\" % \n",
    "      (spell.word_frequency.unique_words, spell.word_frequency.total_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load RMH data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(486458, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SH</th>\n",
       "      <th>SI</th>\n",
       "      <th>length</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>140</td>\n",
       "      <td>SOB for 5/7, been to GP given prednisolone, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107</td>\n",
       "      <td>pt has lac down right forehead, to eyebrow, wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74</td>\n",
       "      <td>pt expect MBA, trapped for 45mins, #right femu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>167</td>\n",
       "      <td>L) sided flank pain same as previous renal col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>193</td>\n",
       "      <td>generalised abdo pain and associated headache ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SH  SI  length                                               text\n",
       "0  0.0 NaN     140  SOB for 5/7, been to GP given prednisolone, co...\n",
       "1  0.0 NaN     107  pt has lac down right forehead, to eyebrow, wi...\n",
       "2  0.0 NaN      74  pt expect MBA, trapped for 45mins, #right femu...\n",
       "3  0.0 NaN     167  L) sided flank pain same as previous renal col...\n",
       "4  0.0 NaN     193  generalised abdo pain and associated headache ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/rmh_raw.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess and tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52 s, sys: 96.9 ms, total: 52.1 s\n",
      "Wall time: 52.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preprocess comments\n",
    "df['text_clean'] = df.text.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scispacy model for tokenization\n",
    "nlp = spacy.load(\"en_core_sci_sm\", disable=['tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner'])\n",
    "nlp.tokenizer = combined_rule_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 7s, sys: 29.7 s, total: 9min 37s\n",
      "Wall time: 9min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['text_clean'] = list(nlp.pipe(df.text_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select a subset of tokens present in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain-specific vocabulary contains 36506 unique words (9127336 words in total).\n"
     ]
    }
   ],
   "source": [
    "# Checks if a token is known and add it to the vocab\n",
    "def add_to_vocab(text):\n",
    "    vocab.extend(spell.known([token.text for token in text])) \n",
    "    \n",
    "# Apply the function to each triage comment\n",
    "vocab = []\n",
    "df.text_clean.apply(add_to_vocab)\n",
    "\n",
    "print(\"Domain-specific vocabulary contains %d unique words (%d words in total).\" % \n",
    "      (len(set(vocab)), len(vocab)))\n",
    "      \n",
    "with open('../data/spelling_correction/rmh_custom_vocab.txt', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

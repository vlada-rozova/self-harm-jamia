{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "import pickle\n",
    "import scispacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span, Doc\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from scispacy.linking import EntityLinker\n",
    "from negspacy.negation import Negex\n",
    "from negspacy.termsets import termset\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57681, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:15:00</td>\n",
       "      <td>argument with friend, threatened to jump off balcony. had voices in her head for 2/12. nil visua...</td>\n",
       "      <td>112</td>\n",
       "      <td>argument with friend, threatened to jump off balcony. had voices in her head for 2/12. nil visua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 00:20:00</td>\n",
       "      <td>mech fall with swelling to L) hand and dec ROM. ETOH intake.</td>\n",
       "      <td>60</td>\n",
       "      <td>mech fall with swelling to left hand and dec rom. etoh intake.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01 00:33:00</td>\n",
       "      <td>Left lower dental pain since last year, seeking analgsia until able to attend dental hospital in...</td>\n",
       "      <td>142</td>\n",
       "      <td>left lower dental pain since last year, seeking analgesia until able to attend dental hospital i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01 00:34:00</td>\n",
       "      <td>ETOH, scuffle with HS ? LOC, lac approx 2cm above R eyebrow will require sutures. GCS 15, full p...</td>\n",
       "      <td>145</td>\n",
       "      <td>etoh, scuffle with hs ? loc, lac approx 2cm above r eyebrow will require sutures. gcs 15, full p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01 00:36:00</td>\n",
       "      <td>mech fall landed L) hip. headstrike onto wall. pain to L) hip, rotation/shortening.</td>\n",
       "      <td>84</td>\n",
       "      <td>mech fall landed left hip. headstrike onto wall. pain to left hip, rotation/shortening.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  \\\n",
       "0  2020-01-01 00:15:00   \n",
       "1  2020-01-01 00:20:00   \n",
       "2  2020-01-01 00:33:00   \n",
       "3  2020-01-01 00:34:00   \n",
       "4  2020-01-01 00:36:00   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  argument with friend, threatened to jump off balcony. had voices in her head for 2/12. nil visua...   \n",
       "1                                         mech fall with swelling to L) hand and dec ROM. ETOH intake.   \n",
       "2  Left lower dental pain since last year, seeking analgsia until able to attend dental hospital in...   \n",
       "3  ETOH, scuffle with HS ? LOC, lac approx 2cm above R eyebrow will require sutures. GCS 15, full p...   \n",
       "4                 mech fall landed L) hip. headstrike onto wall. pain to L) hip, rotation/shortening.    \n",
       "\n",
       "   length  \\\n",
       "0     112   \n",
       "1      60   \n",
       "2     142   \n",
       "3     145   \n",
       "4      84   \n",
       "\n",
       "                                                                                            text_clean  \n",
       "0  argument with friend, threatened to jump off balcony. had voices in her head for 2/12. nil visua...  \n",
       "1                                       mech fall with swelling to left hand and dec rom. etoh intake.  \n",
       "2  left lower dental pain since last year, seeking analgesia until able to attend dental hospital i...  \n",
       "3  etoh, scuffle with hs ? loc, lac approx 2cm above r eyebrow will require sutures. gcs 15, full p...  \n",
       "4              mech fall landed left hip. headstrike onto wall. pain to left hip, rotation/shortening.  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/epic2020_cleaned.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"custom_ner\") \n",
    "def custom_ner(doc):\n",
    "    ents = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct and not token.like_num and token.text!=\"+\":\n",
    "            ents.append(Span(doc, token.i, token.i+1, label=\"ENTITY\"))\n",
    "    doc.ents = ents\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP pipeline: tokenizer + ['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'custom_ner']\n"
     ]
    }
   ],
   "source": [
    "# Load scispacy model\n",
    "nlp = spacy.load(\"en_core_sci_lg\", disable=['ner'])\n",
    "\n",
    "# Custom NER \n",
    "nlp.add_pipe(\"custom_ner\", last=True)\n",
    "\n",
    "print(\"NLP pipeline: tokenizer + {}\".format(nlp.pipe_names))\n",
    "\n",
    "# Modify negex termsets\n",
    "ts = termset('en_clinical').get_patterns()\n",
    "ts['preceding_negations'].extend([\"nil\", \"non\"])\n",
    "ts['termination'].extend([\",\", \";\", \":\", \"obviously\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 14s, sys: 819 ms, total: 5min 15s\n",
      "Wall time: 5min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['doc'] = df.text_clean.apply(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of common bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(data):\n",
    "    vectorizer = CountVectorizer(stop_words=stopwords.words('english') + ts['preceding_negations'], \n",
    "                                 ngram_range=(2,2), \n",
    "                                 token_pattern=r'\\S+')\n",
    "    vectors = vectorizer.fit_transform(data)\n",
    "    \n",
    "    vocab = list(vectorizer.get_feature_names())\n",
    "    counts = vectors.sum(axis=0).A1\n",
    "    \n",
    "    return Counter(dict(zip(vocab, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = word_count(df.doc.apply(lambda x: \" \".join([ent.text for ent in x.ents])))\n",
    "len(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_off = np.quantile(np.fromiter(bigrams.values(), dtype=np.int), 0.99) \n",
    "print(\"Cut-off:\", cut_off)\n",
    "n_bigrams = (np.fromiter(bigrams.values(), dtype=np.int) > cut_off).sum()\n",
    "print(\"%d most common bigrams\" % n_bigrams)\n",
    "most_common_bigrams = [item[0] for item in bigrams.most_common(n_bigrams)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('most_common_bigrams.txt', 'wb') as f:\n",
    "    pickle.dump(most_common_bigrams, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The rest of NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('../data/most_common_bigrams.txt', 'rb') as f:\n",
    "    most_common_bigrams = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_detector(doc):\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end) for _, start, end in matches]\n",
    "    filtered = filter_spans(spans)\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for span in filtered:\n",
    "            retokenizer.merge(span)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_canonical_name(span):\n",
    "    if span._.kb_ents:\n",
    "        concept = linker.kb.cui_to_entity[span._.kb_ents[0][0]].canonical_name.lower()\n",
    "        return re.sub(\"\\W\", \"_\", concept)\n",
    "    else:\n",
    "        return span.text\n",
    "    \n",
    "def format_merged_tokens(span):\n",
    "    return re.sub(\"\\s\", \"_\", span.text)\n",
    "\n",
    "def apply_transformation(span, transform=\"\"):\n",
    "    if transform == \"linked\":\n",
    "        return span._.linked\n",
    "    elif transform == \"merged\":\n",
    "        return span._.merged\n",
    "    else:\n",
    "        return span.text\n",
    "\n",
    "def add_negation(span, transform=\"\"):\n",
    "    return span._.negex * \"neg_\" + span._.transformed(transform)\n",
    "    \n",
    "def prepare_tokens(doc, negation=False, transform=\"\"):\n",
    "    if negation:\n",
    "        return \" \".join([ent._.negated(transform) for ent in doc.ents])\n",
    "    else:\n",
    "        return \" \".join([ent._.transformed(transform) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP pipeline: tokenizer + ['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'custom_ner']\n"
     ]
    }
   ],
   "source": [
    "bigram_patterns = list(nlp.pipe(most_common_bigrams))\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"BIGRAM\", None, *bigram_patterns)\n",
    "\n",
    "# Bigram detector\n",
    "# nlp.add_pipe(bigram_detector, last=True)\n",
    "\n",
    "# Entity linker\n",
    "# linker = EntityLinker(name=\"mesh\", threshold=0.9)\n",
    "# nlp.add_pipe(linker, last=True)\n",
    "\n",
    "# Negation detector\n",
    "# nlp.add_pipe(\"negex\", config={'ent_types': ['ENTITY'], \n",
    "#                               'neg_termset':{\n",
    "#             preceding_negations\": [\"not\"],\n",
    "#             \"following_negations\":[\"declined\"],\n",
    "#             \"termination\": [\"but\",\"however\"]\n",
    "#         }\n",
    "#     }\n",
    "#     )\n",
    "\n",
    "print(\"NLP pipeline: tokenizer + {}\".format(nlp.pipe_names))\n",
    "\n",
    "Span.set_extension(\"linked\", getter=get_canonical_name, force=True)\n",
    "Span.set_extension(\"merged\", getter=format_merged_tokens, force=True)\n",
    "Span.set_extension(\"transformed\", method=apply_transformation, force=True)\n",
    "Span.set_extension(\"negated\", method=add_negation, force=True)\n",
    "Doc.set_extension(\"entities\", method=prepare_tokens, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entities'] = df.doc.apply(lambda x: x._.entities())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entities and negated entities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with nlp.disable_pipes([\"bigram_detector\", \"EntityLinker\"]):\n",
    "    df['doc'] = df.text_clean.apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entities'] = df.doc.apply(lambda x: x._.entities())\n",
    "df['neg_entities'] = df.doc.apply(lambda x: x._.entities(negation=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"doc\").to_csv(\"../data/epic2020_prepared_ents.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merged entities and negated merged entities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with nlp.disable_pipes([\"EntityLinker\"]):\n",
    "    df['doc'] = df.text_clean.apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['merged_entities'] = df.doc.apply(lambda x: x._.entities(transform=\"merged\"))\n",
    "df['neg_merged_entities'] = df.doc.apply(lambda x: x._.entities(negation=True, transform=\"merged\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"doc\").to_csv(\"./data/rmh_prepared_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linked entities and negated linked entities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with nlp.disable_pipes([\"bigram_detector\"]):\n",
    "    df['doc'] = df.text_clean.apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['linked_entities'] = df.doc.apply(lambda x: x._.entities(transform=\"linked\"))\n",
    "df['neg_linked_entities'] = df.doc.apply(lambda x: x._.entities(negation=True, transform=\"linked\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"doc\").to_csv(\"./data/rmh_prepared_linked.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"./data/rmh_prepared_linked_1.csv\")\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"./data/rmh_prepared_linked_2.csv\")\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"./data/rmh_prepared_linked_3.csv\")\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1,df2, df3], axis=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./data/rmh_prepared_linked.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"./data/rmh_prepared_ents.csv\")\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"./data/rmh_prepared_merged.csv\")\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.merge(df2[[\"merged_entities\", \"neg_merged_entities\"]], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"./data/rmh_prepared_linked.csv\")\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df3[[\"linked_entities\", \"neg_linked_entities\"]], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./data/rmh_prepared.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df_.loc[3, \"doc\"]\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umls_entity(ent):\n",
    "    if ent._.kb_ents:\n",
    "        return linker.kb.cui_to_entity[ent._.kb_ents]\n",
    "    else:\n",
    "        return ent.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(df.loc[10000, 'doc'])\n",
    "print(doc, \"\\n\")\n",
    "for token in doc:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent._.negex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    if ent._.kb_ents:\n",
    "        print(\"\\nEntity: \\\"{}\\\", number of linked concepts: {}\".format(ent, len(ent._.kb_ents)))\n",
    "#     print(ent.text, ent._.negex)\n",
    "#     print(canonical_name(ent), \"\\n\")\n",
    "        for concept in ent._.kb_ents:\n",
    "            print(\"\\n\", linker.kb.cui_to_entity[concept[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Significant bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_utils import get_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./data/rmh_train.csv\")\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"entities\"\n",
    "vectorizer_mode = \"select features\"\n",
    "params = {'analyzer' : \"word\",\n",
    "          'ngram_range' : (2,2),\n",
    "          'use_idf' : True,\n",
    "          'mode' : \"select by pvalue\",\n",
    "          'thresh' : 0.0001}\n",
    "\n",
    "vectorizer = get_vectorizer(vectorizer_mode, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.SH.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(df_train[text], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.df_features.p_value.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "import pickle\n",
    "import scispacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span, Doc\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from scispacy.linking import EntityLinker\n",
    "from negspacy.negation import Negex\n",
    "from negspacy.termsets import termset\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/rmh_cleaned.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"custom_ner\") \n",
    "def custom_ner(doc):\n",
    "    ents = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct and not token.like_num and token.text!=\"+\":\n",
    "            ents.append(Span(doc, token.i, token.i+1, label=\"ENTITY\"))\n",
    "    doc.ents = ents\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scispacy model\n",
    "nlp = spacy.load(\"en_core_sci_lg\", disable=['ner'])\n",
    "\n",
    "# Custom NER \n",
    "nlp.add_pipe(\"custom_ner\", last=True)\n",
    "\n",
    "print(\"NLP pipeline: tokenizer + {}\".format(nlp.pipe_names))\n",
    "\n",
    "# Modify negex termsets\n",
    "ts = termset('en_clinical').get_patterns()\n",
    "ts['preceding_negations'].extend([\"nil\", \"non\"])\n",
    "ts['termination'].extend([\",\", \";\", \":\", \"obviously\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['doc'] = df.text_clean.apply(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of common bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(data):\n",
    "    vectorizer = CountVectorizer(stop_words=stopwords.words('english') + ts['preceding_negations'], \n",
    "                                 ngram_range=(2,2), \n",
    "                                 token_pattern=r'\\S+')\n",
    "    vectors = vectorizer.fit_transform(data)\n",
    "    \n",
    "    vocab = list(vectorizer.get_feature_names())\n",
    "    counts = vectors.sum(axis=0).A1\n",
    "    \n",
    "    return Counter(dict(zip(vocab, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = word_count(df.doc.apply(lambda x: \" \".join([ent.text for ent in x.ents])))\n",
    "len(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_off = np.quantile(np.fromiter(bigrams.values(), dtype=np.int), 0.99) \n",
    "print(\"Cut-off:\", cut_off)\n",
    "n_bigrams = (np.fromiter(bigrams.values(), dtype=np.int) > cut_off).sum()\n",
    "print(\"%d most common bigrams\" % n_bigrams)\n",
    "most_common_bigrams = [item[0] for item in bigrams.most_common(n_bigrams)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../../data/most_common_bigrams.txt\", 'wb') as f:\n",
    "    pickle.dump(most_common_bigrams, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The rest of NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../../data/most_common_bigrams.txt\", 'rb') as f:\n",
    "    most_common_bigrams = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_detector(doc):\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end) for _, start, end in matches]\n",
    "    filtered = filter_spans(spans)\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for span in filtered:\n",
    "            retokenizer.merge(span)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_canonical_name(span):\n",
    "    if span._.kb_ents:\n",
    "        concept = linker.kb.cui_to_entity[span._.kb_ents[0][0]].canonical_name.lower()\n",
    "        return re.sub(\"\\W\", \"_\", concept)\n",
    "    else:\n",
    "        return span.text\n",
    "    \n",
    "def format_merged_tokens(span):\n",
    "    return re.sub(\"\\s\", \"_\", span.text)\n",
    "\n",
    "def apply_transformation(span, transform=\"\"):\n",
    "    if transform == \"linked\":\n",
    "        return span._.linked\n",
    "    elif transform == \"merged\":\n",
    "        return span._.merged\n",
    "    else:\n",
    "        return span.text\n",
    "\n",
    "def add_negation(span, transform=\"\"):\n",
    "    return span._.negex * \"neg_\" + span._.transformed(transform)\n",
    "    \n",
    "def prepare_tokens(doc, negation=False, transform=\"\"):\n",
    "    if negation:\n",
    "        return \" \".join([ent._.negated(transform) for ent in doc.ents])\n",
    "    else:\n",
    "        return \" \".join([ent._.transformed(transform) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_patterns = list(nlp.pipe(most_common_bigrams))\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"BIGRAM\", None, *bigram_patterns)\n",
    "\n",
    "# Bigram detector\n",
    "nlp.add_pipe(bigram_detector, last=True)\n",
    "\n",
    "# Entity linker\n",
    "linker = EntityLinker(name=\"mesh\", threshold=0.9)\n",
    "nlp.add_pipe(linker, last=True)\n",
    "\n",
    "# Negation detector\n",
    "nlp.add_pipe(\"negex\", config={'ent_types': ['ENTITY'], \n",
    "                              'neg_termset':{\n",
    "            preceding_negations\": [\"not\"],\n",
    "            \"following_negations\":[\"declined\"],\n",
    "            \"termination\": [\"but\",\"however\"]\n",
    "        }\n",
    "    }\n",
    "    )\n",
    "\n",
    "print(\"NLP pipeline: tokenizer + {}\".format(nlp.pipe_names))\n",
    "\n",
    "Span.set_extension(\"linked\", getter=get_canonical_name, force=True)\n",
    "Span.set_extension(\"merged\", getter=format_merged_tokens, force=True)\n",
    "Span.set_extension(\"transformed\", method=apply_transformation, force=True)\n",
    "Span.set_extension(\"negated\", method=add_negation, force=True)\n",
    "Doc.set_extension(\"entities\", method=prepare_tokens, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entities and negated entities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with nlp.disable_pipes([\"bigram_detector\", \"EntityLinker\"]):\n",
    "    df['doc'] = df.text_clean.apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entities'] = df.doc.apply(lambda x: x._.entities())\n",
    "df['neg_entities'] = df.doc.apply(lambda x: x._.entities(negation=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"doc\").to_csv(\"../../data/rmh_prepared_ents.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merged entities and negated merged entities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with nlp.disable_pipes([\"EntityLinker\"]):\n",
    "    df['doc'] = df.text_clean.apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['merged_entities'] = df.doc.apply(lambda x: x._.entities(transform=\"merged\"))\n",
    "df['neg_merged_entities'] = df.doc.apply(lambda x: x._.entities(negation=True, transform=\"merged\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"doc\").to_csv(\"../../data/rmh_prepared_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linked entities and negated linked entities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with nlp.disable_pipes([\"bigram_detector\"]):\n",
    "    df['doc'] = df.text_clean.apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['linked_entities'] = df.doc.apply(lambda x: x._.entities(transform=\"linked\"))\n",
    "df['neg_linked_entities'] = df.doc.apply(lambda x: x._.entities(negation=True, transform=\"linked\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"doc\").to_csv(\"../../data/rmh_prepared_linked.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"../../data/rmh_prepared_ents.csv\")\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"../../data/rmh_prepared_merged.csv\")\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.merge(df2[[\"merged_entities\", \"neg_merged_entities\"]], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"../../data/rmh_prepared_linked.csv\")\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df3[[\"linked_entities\", \"neg_linked_entities\"]], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../data/rmh_prepared.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
